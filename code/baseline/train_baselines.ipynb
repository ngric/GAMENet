{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from util import multi_label_metric\n",
    "\n",
    "data_path = '../../data/records_final.pkl'\n",
    "voc_path = '../../data/voc_final.pkl'\n",
    "\n",
    "ddi_adj_path = '../../data/ddi_A_final.pkl'\n",
    "\n",
    "data = dill.load(open(data_path, 'rb'))\n",
    "voc = dill.load(open(voc_path, 'rb'))\n",
    "diag_voc, pro_voc, med_voc = voc['diag_voc'], voc['pro_voc'], voc['med_voc']\n",
    "split_point = int(len(data) * 2 / 3)\n",
    "data_train = data[:split_point]\n",
    "eval_len = int(len(data[split_point:]) / 2)\n",
    "data_test = data[split_point:split_point + eval_len]\n",
    "data_eval = data[split_point+eval_len:]\n",
    "\n",
    "\n",
    "def main():\n",
    "    gt = []\n",
    "    pred = []\n",
    "    for patient in data_test:\n",
    "        if len(patient) == 1:\n",
    "            continue\n",
    "        for adm_idx, adm in enumerate(patient):\n",
    "            if adm_idx < len(patient)-1:\n",
    "                gt.append(patient[adm_idx+1][2])\n",
    "                pred.append(adm[2])\n",
    "    med_voc_size = len(med_voc.idx2word)\n",
    "    y_gt = np.zeros((len(gt), med_voc_size))\n",
    "    y_pred = np.zeros((len(gt), med_voc_size))\n",
    "    for idx, item in enumerate(gt):\n",
    "        y_gt[idx, item] = 1\n",
    "    for idx, item in enumerate(pred):\n",
    "        y_pred[idx, item] = 1\n",
    "\n",
    "    ja, prauc, avg_p, avg_r, avg_f1 = multi_label_metric(y_gt, y_pred, y_pred)\n",
    "\n",
    "    # ddi rate\n",
    "    ddi_A = dill.load(open(ddi_adj_path, 'rb'))\n",
    "    all_cnt = 0\n",
    "    dd_cnt = 0\n",
    "    med_cnt = 0\n",
    "    visit_cnt = 0\n",
    "    for adm in y_pred:\n",
    "        med_code_set = np.where(adm == 1)[0]\n",
    "        visit_cnt += 1\n",
    "        med_cnt += len(med_code_set)\n",
    "        for i, med_i in enumerate(med_code_set):\n",
    "            for j, med_j in enumerate(med_code_set):\n",
    "                if j <= i:\n",
    "                    continue\n",
    "                all_cnt += 1\n",
    "                if ddi_A[med_i, med_j] == 1 or ddi_A[med_j, med_i] == 1:\n",
    "                    dd_cnt += 1\n",
    "    ddi_rate = dd_cnt / all_cnt\n",
    "    print('\\tDDI Rate: %.4f, Jaccard: %.4f,  PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n",
    "        ddi_rate, ja, prauc, avg_p, avg_r, avg_f1\n",
    "    ))\n",
    "    print('avg med', med_cnt/ visit_cnt)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import jaccard_score\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from util import multi_label_metric\n",
    "\n",
    "np.random.seed(1203)\n",
    "model_name = 'LR'\n",
    "\n",
    "if not os.path.exists(os.path.join(\"saved\", model_name)):\n",
    "    os.makedirs(os.path.join(\"saved\", model_name))\n",
    "\n",
    "def create_dataset(data, diag_voc, pro_voc, med_voc):\n",
    "    i1_len = len(diag_voc.idx2word)\n",
    "    i2_len = len(pro_voc.idx2word)\n",
    "    output_len = len(med_voc.idx2word)\n",
    "    input_len = i1_len + i2_len\n",
    "    X = []\n",
    "    y = []\n",
    "    for patient in data:\n",
    "        for visit in patient:\n",
    "            i1 = visit[0]\n",
    "            i2 = visit[1]\n",
    "            o = visit[2]\n",
    "\n",
    "            multi_hot_input = np.zeros(input_len)\n",
    "            multi_hot_input[i1] = 1\n",
    "            multi_hot_input[np.array(i2) + i1_len] = 1\n",
    "\n",
    "            multi_hot_output = np.zeros(output_len)\n",
    "            multi_hot_output[o] = 1\n",
    "\n",
    "            X.append(multi_hot_input)\n",
    "            y.append(multi_hot_output)\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def main():\n",
    "    grid_search = False\n",
    "    data_path = '../../data/records_final.pkl'\n",
    "    voc_path = '../../data/voc_final.pkl'\n",
    "\n",
    "    data = dill.load(open(data_path, 'rb'))\n",
    "    voc = dill.load(open(voc_path, 'rb'))\n",
    "    diag_voc, pro_voc, med_voc = voc['diag_voc'], voc['pro_voc'], voc['med_voc']\n",
    "\n",
    "    split_point = int(len(data) * 2 / 3)\n",
    "    data_train = data[:split_point]\n",
    "    eval_len = int(len(data[split_point:]) / 2)\n",
    "    data_eval = data[split_point+eval_len:]\n",
    "    data_test = data[split_point:split_point + eval_len]\n",
    "\n",
    "    train_X, train_y = create_dataset(data_train, diag_voc, pro_voc, med_voc)\n",
    "    test_X, test_y = create_dataset(data_test, diag_voc, pro_voc, med_voc)\n",
    "    eval_X, eval_y = create_dataset(data_eval, diag_voc, pro_voc, med_voc)\n",
    "\n",
    "    if grid_search:\n",
    "        params = {\n",
    "            'estimator__penalty': ['l2'],\n",
    "            'estimator__C': np.linspace(0.00002, 1, 100)\n",
    "        }\n",
    "\n",
    "        model = LogisticRegression()\n",
    "        classifier = OneVsRestClassifier(model)\n",
    "        lr_gs = GridSearchCV(classifier, params, verbose=1).fit(train_X, train_y)\n",
    "\n",
    "        print(\"Best Params\", lr_gs.best_params_)\n",
    "        print(\"Best Score\", lr_gs.best_score_)\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    # sample_X, sample_y = create_dataset(sample_data, diag_voc, pro_voc, med_voc)\n",
    "\n",
    "    model = LogisticRegression(C=0.90909)\n",
    "    classifier = OneVsRestClassifier(model)\n",
    "    classifier.fit(train_X, train_y)\n",
    "\n",
    "    y_pred = classifier.predict(test_X)\n",
    "    y_prob = classifier.predict_proba(test_X)\n",
    "\n",
    "    ja, prauc, avg_p, avg_r, avg_f1 = multi_label_metric(test_y, y_pred, y_prob)\n",
    "\n",
    "    # ddi rate\n",
    "    ddi_A = dill.load(open('../../data/ddi_A_final.pkl', 'rb'))\n",
    "    all_cnt = 0\n",
    "    dd_cnt = 0\n",
    "    med_cnt = 0\n",
    "    visit_cnt = 0\n",
    "    for adm in y_pred:\n",
    "        med_code_set = np.where(adm==1)[0]\n",
    "        visit_cnt += 1\n",
    "        med_cnt += len(med_code_set)\n",
    "        for i, med_i in enumerate(med_code_set):\n",
    "            for j, med_j in enumerate(med_code_set):\n",
    "                if j <= i:\n",
    "                    continue\n",
    "                all_cnt += 1\n",
    "                if ddi_A[med_i, med_j] == 1 or ddi_A[med_j, med_i] == 1:\n",
    "                    dd_cnt += 1\n",
    "    ddi_rate = dd_cnt / all_cnt\n",
    "    print('\\tDDI Rate: %.4f, Jaccard: %.4f, PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n",
    "        ddi_rate, ja, prauc, avg_p, avg_r, avg_f1\n",
    "    ))\n",
    "\n",
    "    history = defaultdict(list)\n",
    "    for i in range(30):\n",
    "        history['jaccard'].append(ja)\n",
    "        history['ddi_rate'].append(ddi_rate)\n",
    "        history['avg_p'].append(avg_p)\n",
    "        history['avg_r'].append(avg_r)\n",
    "        history['avg_f1'].append(avg_f1)\n",
    "        history['prauc'].append(prauc)\n",
    "\n",
    "    dill.dump(history, open(os.path.join('saved', model_name, 'history.pkl'), 'wb'))\n",
    "\n",
    "    print('avg med', med_cnt / visit_cnt)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import jaccard_score, roc_auc_score, precision_score, f1_score, average_precision_score\n",
    "import numpy as np\n",
    "import dill\n",
    "import time\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from models import Leap\n",
    "from util import llprint, sequence_metric, sequence_output_process, ddi_rate_score, get_n_params\n",
    "\n",
    "torch.manual_seed(1203)\n",
    "\n",
    "model_name = 'Leap'\n",
    "resume_name = ''\n",
    "\n",
    "def eval(model, data_eval, voc_size, epoch):\n",
    "    # evaluate\n",
    "    print('')\n",
    "    model.eval()\n",
    "\n",
    "    ja, prauc, avg_p, avg_r, avg_f1 = [[] for _ in range(5)]\n",
    "    records = []\n",
    "    med_cnt = 0\n",
    "    visit_cnt = 0\n",
    "    for step, input in enumerate(data_eval):\n",
    "        y_gt = []\n",
    "        y_pred = []\n",
    "        y_pred_prob = []\n",
    "        y_pred_label = []\n",
    "        for adm in input:\n",
    "            y_gt_tmp = np.zeros(voc_size[2])\n",
    "            y_gt_tmp[adm[2]] = 1\n",
    "            y_gt.append(y_gt_tmp)\n",
    "\n",
    "            output_logits = model(adm)\n",
    "            output_logits = output_logits.detach().cpu().numpy()\n",
    "\n",
    "            out_list, sorted_predict = sequence_output_process(output_logits, [voc_size[2], voc_size[2]+1])\n",
    "\n",
    "            y_pred_label.append(sorted(sorted_predict))\n",
    "            y_pred_prob.append(np.mean(output_logits[:, :-2], axis=0))\n",
    "\n",
    "            y_pred_tmp = np.zeros(voc_size[2])\n",
    "            y_pred_tmp[out_list] = 1\n",
    "            y_pred.append(y_pred_tmp)\n",
    "            visit_cnt += 1\n",
    "            med_cnt += len(sorted_predict)\n",
    "        records.append(y_pred_label)\n",
    "\n",
    "        adm_ja, adm_prauc, adm_avg_p, adm_avg_r, adm_avg_f1 = sequence_metric(np.array(y_gt), np.array(y_pred), np.array(y_pred_prob), np.array(y_pred_label))\n",
    "        ja.append(adm_ja)\n",
    "        prauc.append(adm_prauc)\n",
    "        avg_p.append(adm_avg_p)\n",
    "        avg_r.append(adm_avg_r)\n",
    "        avg_f1.append(adm_avg_f1)\n",
    "        llprint('\\rEval--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_eval)))\n",
    "\n",
    "    # ddi rate\n",
    "    ddi_rate = ddi_rate_score(records)\n",
    "    llprint('\\tDDI Rate: %.4f, Jaccard: %.4f,  PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n",
    "        ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
    "    ))\n",
    "    print('avg med', med_cnt / visit_cnt)\n",
    "    return ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(os.path.join(\"saved\", model_name)):\n",
    "        os.makedirs(os.path.join(\"saved\", model_name))\n",
    "\n",
    "    data_path = '../../data/records_final.pkl'\n",
    "    voc_path = '../../data/voc_final.pkl'\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "    data = dill.load(open(data_path, 'rb'))\n",
    "    voc = dill.load(open(voc_path, 'rb'))\n",
    "    diag_voc, pro_voc, med_voc = voc['diag_voc'], voc['pro_voc'], voc['med_voc']\n",
    "\n",
    "\n",
    "    split_point = int(len(data) * 2 / 3)\n",
    "    data_train = data[:split_point]\n",
    "    eval_len = int(len(data[split_point:]) / 2)\n",
    "    data_test = data[split_point:split_point + eval_len]\n",
    "    data_eval = data[split_point+eval_len:]\n",
    "    voc_size = (len(diag_voc.idx2word), len(pro_voc.idx2word), len(med_voc.idx2word))\n",
    "\n",
    "    EPOCH = 30\n",
    "    LR = 0.0002\n",
    "    TEST = False\n",
    "    END_TOKEN = voc_size[2] + 1\n",
    "\n",
    "    model = Leap(voc_size, device=device)\n",
    "    if TEST:\n",
    "        model.load_state_dict(torch.load(open(os.path.join(\"saved\", model_name, resume_name), 'rb')))\n",
    "        # pass\n",
    "\n",
    "    model.to(device=device)\n",
    "    print('parameters', get_n_params(model))\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    if TEST:\n",
    "        eval(model, data_test, voc_size, 0)\n",
    "    else:\n",
    "        history = defaultdict(list)\n",
    "        for epoch in range(EPOCH):\n",
    "            loss_record = []\n",
    "            start_time = time.time()\n",
    "            model.train()\n",
    "            for step, input in enumerate(data_train):\n",
    "                for adm in input:\n",
    "                    loss_target = adm[2] + [END_TOKEN]\n",
    "                    output_logits = model(adm)\n",
    "                    loss = F.cross_entropy(output_logits, torch.LongTensor(loss_target).to(device))\n",
    "\n",
    "                    loss_record.append(loss.item())\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    optimizer.step()\n",
    "\n",
    "                llprint('\\rTrain--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_train)))\n",
    "\n",
    "            ddi_rate, ja, prauc, avg_p, avg_r, avg_f1 = eval(model, data_eval, voc_size, epoch)\n",
    "            history['ja'].append(ja)\n",
    "            history['ddi_rate'].append(ddi_rate)\n",
    "            history['avg_p'].append(avg_p)\n",
    "            history['avg_r'].append(avg_r)\n",
    "            history['avg_f1'].append(avg_f1)\n",
    "            history['prauc'].append(prauc)\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = (end_time - start_time) / 60\n",
    "            llprint('\\tEpoch: %d, Loss1: %.4f, One Epoch Time: %.2fm, Appro Left Time: %.2fh\\n' % (epoch,\n",
    "                                                                                                np.mean(loss_record),\n",
    "                                                                                                elapsed_time,\n",
    "                                                                                                elapsed_time * (\n",
    "                                                                                                            EPOCH - epoch - 1)/60))\n",
    "\n",
    "            torch.save(model.state_dict(), open( os.path.join('saved', model_name, 'Epoch_%d_JA_%.4f_DDI_%.4f.model' % (epoch, ja, ddi_rate)), 'wb'))\n",
    "            print('')\n",
    "\n",
    "        dill.dump(history, open(os.path.join('saved', model_name, 'history.pkl'), 'wb'))\n",
    "        # test\n",
    "        torch.save(model.state_dict(), open(\n",
    "            os.path.join('saved', model_name, 'final.model'), 'wb'))\n",
    "\n",
    "def fine_tune(fine_tune_name=''):\n",
    "    data_path = '../../data/records_final.pkl'\n",
    "    voc_path = '../../data/voc_final.pkl'\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "    data = dill.load(open(data_path, 'rb'))\n",
    "    voc = dill.load(open(voc_path, 'rb'))\n",
    "    diag_voc, pro_voc, med_voc = voc['diag_voc'], voc['pro_voc'], voc['med_voc']\n",
    "    ddi_A = dill.load(open('../../data/ddi_A_final.pkl', 'rb'))\n",
    "\n",
    "    split_point = int(len(data) * 2 / 3)\n",
    "    data_train = data[:split_point]\n",
    "    eval_len = int(len(data[split_point:]) / 2)\n",
    "    data_test = data[split_point:split_point + eval_len]\n",
    "    # data_eval = data[split_point+eval_len:]\n",
    "    voc_size = (len(diag_voc.idx2word), len(pro_voc.idx2word), len(med_voc.idx2word))\n",
    "\n",
    "    model = Leap(voc_size, device=device)\n",
    "    model.load_state_dict(torch.load(open(os.path.join(\"saved\", model_name, fine_tune_name), 'rb')))\n",
    "    model.to(device)\n",
    "\n",
    "    EPOCH = 30\n",
    "    LR = 0.0001\n",
    "    END_TOKEN = voc_size[2] + 1\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    ddi_rate_record = []\n",
    "    for epoch in range(1):\n",
    "        loss_record = []\n",
    "        start_time = time.time()\n",
    "        random_train_set = [ random.choice(data_train) for i in range(len(data_train))]\n",
    "        for step, input in enumerate(random_train_set):\n",
    "            model.train()\n",
    "            K_flag = False\n",
    "            for adm in input:\n",
    "                target = adm[2]\n",
    "                output_logits = model(adm)\n",
    "                out_list, sorted_predict = sequence_output_process(output_logits.detach().cpu().numpy(), [voc_size[2], voc_size[2] + 1])\n",
    "\n",
    "                inter = set(out_list) & set(target)\n",
    "                union = set(out_list) | set(target)\n",
    "                jaccard = 0 if union == 0 else len(inter) / len(union)\n",
    "                K = 0\n",
    "                for i in out_list:\n",
    "                    if K == 1:\n",
    "                        K_flag = True\n",
    "                        break\n",
    "                    for j in out_list:\n",
    "                        if ddi_A[i][j] == 1:\n",
    "                            K = 1\n",
    "                            break\n",
    "\n",
    "                loss = -jaccard * K * torch.mean(F.log_softmax(output_logits, dim=-1))\n",
    "\n",
    "\n",
    "                loss_record.append(loss.item())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward(retain_graph=True)\n",
    "                optimizer.step()\n",
    "\n",
    "            llprint('\\rTrain--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_train)))\n",
    "\n",
    "            if K_flag:\n",
    "                ddi_rate, ja, prauc, avg_p, avg_r, avg_f1 = eval(model, data_test, voc_size, epoch)\n",
    "\n",
    "\n",
    "                end_time = time.time()\n",
    "                elapsed_time = (end_time - start_time) / 60\n",
    "                llprint('\\tEpoch: %d, Loss1: %.4f, One Epoch Time: %.2fm, Appro Left Time: %.2fh\\n' % (epoch,\n",
    "                                                                                               np.mean(loss_record),\n",
    "                                                                                               elapsed_time,\n",
    "                                                                                               elapsed_time * (\n",
    "                                                                                                       EPOCH - epoch - 1) / 60))\n",
    "\n",
    "                torch.save(model.state_dict(),\n",
    "                   open(os.path.join('saved', model_name, 'fine_Epoch_%d_JA_%.4f_DDI_%.4f.model' % (epoch, ja, ddi_rate)),\n",
    "                        'wb'))\n",
    "                print('')\n",
    "\n",
    "    # test\n",
    "    torch.save(model.state_dict(), open(\n",
    "        os.path.join('saved', model_name, 'final.model'), 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # main()\n",
    "    fine_tune(fine_tune_name='Epoch_26_JA_0.4465_DDI_0.0723.model')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import jaccard_score, roc_auc_score, precision_score, f1_score, average_precision_score\n",
    "import numpy as np\n",
    "import dill\n",
    "import time\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from models import Retain\n",
    "from util import llprint, multi_label_metric, ddi_rate_score, get_n_params\n",
    "\n",
    "torch.manual_seed(1203)\n",
    "model_name = 'Retain'\n",
    "resume_name = ''\n",
    "\n",
    "def eval(model, data_eval, voc_size, epoch):\n",
    "    # evaluate\n",
    "    print('')\n",
    "    model.eval()\n",
    "    smm_record = []\n",
    "    ja, prauc, avg_p, avg_r, avg_f1 = [[] for _ in range(5)]\n",
    "    case_study = defaultdict(dict)\n",
    "    med_cnt = 0\n",
    "    visit_cnt = 0\n",
    "    for step, input in enumerate(data_eval):\n",
    "        if len(input) < 2: # visit > 2\n",
    "            continue\n",
    "        y_gt = []\n",
    "        y_pred = []\n",
    "        y_pred_prob = []\n",
    "        y_pred_label = []\n",
    "        for i in range(1, len(input)):\n",
    "\n",
    "            y_pred_label_tmp = []\n",
    "            y_gt_tmp = np.zeros(voc_size[2])\n",
    "            y_gt_tmp[input[i][2]] = 1\n",
    "            y_gt.append(y_gt_tmp)\n",
    "\n",
    "            target_output1 = model(input[:i])\n",
    "\n",
    "            target_output1 = F.sigmoid(target_output1).detach().cpu().numpy()[0]\n",
    "            y_pred_prob.append(target_output1)\n",
    "            y_pred_tmp = target_output1.copy()\n",
    "            y_pred_tmp[y_pred_tmp >= 0.3] = 1\n",
    "            y_pred_tmp[y_pred_tmp < 0.3] = 0\n",
    "            y_pred.append(y_pred_tmp)\n",
    "            for idx, value in enumerate(y_pred_tmp):\n",
    "                if value == 1:\n",
    "                    y_pred_label_tmp.append(idx)\n",
    "            y_pred_label.append(y_pred_label_tmp)\n",
    "            med_cnt += len(y_pred_label_tmp)\n",
    "            visit_cnt += 1\n",
    "\n",
    "        smm_record.append(y_pred_label)\n",
    "        adm_ja, adm_prauc, adm_avg_p, adm_avg_r, adm_avg_f1 = multi_label_metric(np.array(y_gt), np.array(y_pred),\n",
    "                                                                                   np.array(y_pred_prob))\n",
    "        case_study[adm_ja] = {'ja': adm_ja, 'patient':input, 'y_label':y_pred_label}\n",
    "        ja.append(adm_ja)\n",
    "        prauc.append(adm_prauc)\n",
    "        avg_p.append(adm_avg_p)\n",
    "        avg_r.append(adm_avg_r)\n",
    "        avg_f1.append(adm_avg_f1)\n",
    "        llprint('\\rEval--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_eval)))\n",
    "\n",
    "    dill.dump(case_study, open(os.path.join('saved', model_name, 'case_study.pkl'), 'wb'))\n",
    "    # ddi rate\n",
    "    ddi_rate = ddi_rate_score(smm_record)\n",
    "\n",
    "    llprint('\\tDDI Rate: %.4f, Jaccard: %.4f,  PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n",
    "        ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
    "    ))\n",
    "    print('avg med', med_cnt / visit_cnt)\n",
    "\n",
    "    return ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(os.path.join(\"saved\", model_name)):\n",
    "        os.makedirs(os.path.join(\"saved\", model_name))\n",
    "\n",
    "    data_path = '../data/records_final.pkl'\n",
    "    voc_path = '../data/voc_final.pkl'\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "    data = dill.load(open(data_path, 'rb'))\n",
    "    voc = dill.load(open(voc_path, 'rb'))\n",
    "    diag_voc, pro_voc, med_voc = voc['diag_voc'], voc['pro_voc'], voc['med_voc']\n",
    "\n",
    "    split_point = int(len(data) * 2 / 3)\n",
    "    data_train = data[:split_point]\n",
    "    eval_len = int(len(data[split_point:]) / 2)\n",
    "    data_test = data[split_point:split_point + eval_len]\n",
    "    data_eval = data[split_point+eval_len:]\n",
    "    voc_size = (len(diag_voc.idx2word), len(pro_voc.idx2word), len(med_voc.idx2word))\n",
    "\n",
    "    EPOCH = 40\n",
    "    LR = 0.0002\n",
    "    TEST = False\n",
    "\n",
    "    model = Retain(voc_size, device=device)\n",
    "    if TEST:\n",
    "        model.load_state_dict(torch.load(open(os.path.join(\"saved\", model_name, resume_name), 'rb')))\n",
    "\n",
    "    model.to(device=device)\n",
    "    print('parameters', get_n_params(model))\n",
    "\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    if TEST:\n",
    "        eval(model, data_test, voc_size, 0)\n",
    "    else:\n",
    "        history = defaultdict(list)\n",
    "        for epoch in range(EPOCH):\n",
    "            loss_record = []\n",
    "            start_time = time.time()\n",
    "            model.train()\n",
    "            for step, input in enumerate(data_train):\n",
    "                if len(input) < 2:\n",
    "                    continue\n",
    "\n",
    "                loss = 0\n",
    "                for i in range(1, len(input)):\n",
    "                    target = np.zeros((1, voc_size[2]))\n",
    "                    target[:, input[i][2]] = 1\n",
    "\n",
    "                    output_logits = model(input[:i])\n",
    "                    loss += F.binary_cross_entropy_with_logits(output_logits, torch.FloatTensor(target).to(device))\n",
    "                    loss_record.append(loss.item())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                llprint('\\rTrain--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_train)))\n",
    "\n",
    "            ddi_rate, ja, prauc, avg_p, avg_r, avg_f1 = eval(model, data_eval, voc_size, epoch)\n",
    "            history['ja'].append(ja)\n",
    "            history['ddi_rate'].append(ddi_rate)\n",
    "            history['avg_p'].append(avg_p)\n",
    "            history['avg_r'].append(avg_r)\n",
    "            history['avg_f1'].append(avg_f1)\n",
    "            history['prauc'].append(prauc)\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = (end_time - start_time) / 60\n",
    "            llprint('\\tEpoch: %d, Loss1: %.4f, One Epoch Time: %.2fm, Appro Left Time: %.2fh\\n' % (epoch,\n",
    "                                                                                                np.mean(loss_record),\n",
    "                                                                                                elapsed_time,\n",
    "                                                                                                elapsed_time * (\n",
    "                                                                                                            EPOCH - epoch - 1)/60))\n",
    "\n",
    "            torch.save(model.state_dict(), open( os.path.join('saved', model_name, 'Epoch_%d_JA_%.4f_DDI_%.4f.model' % (epoch, ja, ddi_rate)), 'wb'))\n",
    "            print('')\n",
    "\n",
    "        dill.dump(history, open(os.path.join('saved', model_name, 'history.pkl'), 'wb'))\n",
    "\n",
    "        # test\n",
    "        torch.save(model.state_dict(), open(\n",
    "            os.path.join('saved', model_name, 'final.model'), 'wb'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import jaccard_score, roc_auc_score, precision_score, f1_score, average_precision_score\n",
    "import numpy as np\n",
    "import dill\n",
    "import time\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from models import DMNC\n",
    "from util import llprint, sequence_metric, ddi_rate_score, get_n_params\n",
    "\n",
    "torch.manual_seed(1203)\n",
    "model_name = 'DMNC'\n",
    "resume_name = ''\n",
    "\n",
    "'''\n",
    "It's better to refer to the offical implement in tensorflow.  https://github.com/thaihungle/DMNC\n",
    "'''\n",
    "\n",
    "def sequence_output_process(output_logits, filter_token):\n",
    "    pind = np.argsort(output_logits, axis=-1)[:, ::-1]\n",
    "    out_list = []\n",
    "    for i in range(len(pind)):\n",
    "        for j in range(pind.shape[1]):\n",
    "            label = pind[i][j]\n",
    "            if label in filter_token:\n",
    "                continue\n",
    "            if label not in out_list:\n",
    "                out_list.append(label)\n",
    "                break\n",
    "    y_pred_prob_tmp = []\n",
    "    for idx, item in enumerate(out_list):\n",
    "        y_pred_prob_tmp.append(output_logits[idx, item])\n",
    "    sorted_predict = [x for _, x in sorted(zip(y_pred_prob_tmp, out_list), reverse=True)]\n",
    "    return out_list, sorted_predict\n",
    "\n",
    "def eval(model, data_eval, voc_size, epoch):\n",
    "    # evaluate\n",
    "    print('')\n",
    "    model.eval()\n",
    "\n",
    "    ja, prauc, avg_p, avg_r, avg_f1 = [[] for _ in range(5)]\n",
    "    records = []\n",
    "    for step, input in enumerate(data_eval):\n",
    "        y_gt = []\n",
    "        y_pred = []\n",
    "        y_pred_prob = []\n",
    "        y_pred_label = []\n",
    "        i1_state, i2_state, i3_state = None, None, None\n",
    "        for adm in input:\n",
    "            y_gt_tmp = np.zeros(voc_size[2])\n",
    "            y_gt_tmp[adm[2]] = 1\n",
    "            y_gt.append(y_gt_tmp)\n",
    "\n",
    "            output_logits, i1_state, i2_state, i3_state = model(adm, i1_state, i2_state, i3_state)\n",
    "            output_logits = output_logits.detach().cpu().numpy()\n",
    "\n",
    "            out_list, sorted_predict = sequence_output_process(output_logits, [voc_size[2], voc_size[2]+1])\n",
    "\n",
    "            y_pred_label.append(sorted_predict)\n",
    "            y_pred_prob.append(np.mean(output_logits[:,:-2], axis=0))\n",
    "\n",
    "            y_pred_tmp = np.zeros(voc_size[2])\n",
    "            y_pred_tmp[out_list] = 1\n",
    "            y_pred.append(y_pred_tmp)\n",
    "        records.append(y_pred_label)\n",
    "\n",
    "        adm_ja, adm_prauc, adm_avg_p, adm_avg_r, adm_avg_f1 = sequence_metric(np.array(y_gt), np.array(y_pred),\n",
    "                                                                              np.array(y_pred_prob),\n",
    "                                                                              np.array(y_pred_label))\n",
    "        ja.append(adm_ja)\n",
    "        prauc.append(adm_prauc)\n",
    "        avg_p.append(adm_avg_p)\n",
    "        avg_r.append(adm_avg_r)\n",
    "        avg_f1.append(adm_avg_f1)\n",
    "\n",
    "        llprint('\\rEval--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_eval)))\n",
    "\n",
    "    # ddi rate\n",
    "    ddi_rate = ddi_rate_score(records)\n",
    "    llprint('\\tDDI Rate: %.4f, Jaccard: %.4f,  PRAUC: %.4f, AVG_PRC: %.4f, AVG_RECALL: %.4f, AVG_F1: %.4f\\n' % (\n",
    "        ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
    "    ))\n",
    "    return ddi_rate, np.mean(ja), np.mean(prauc), np.mean(avg_p), np.mean(avg_r), np.mean(avg_f1)\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(os.path.join(\"saved\", model_name)):\n",
    "        os.makedirs(os.path.join(\"saved\", model_name))\n",
    "\n",
    "    data_path = '../data/records_final.pkl'\n",
    "    voc_path = '../data/voc_final.pkl'\n",
    "    device = torch.device('cuda:0')\n",
    "\n",
    "    data = dill.load(open(data_path, 'rb'))\n",
    "    voc = dill.load(open(voc_path, 'rb'))\n",
    "    diag_voc, pro_voc, med_voc = voc['diag_voc'], voc['pro_voc'], voc['med_voc']\n",
    "\n",
    "    split_point = int(len(data) * 2 / 3)\n",
    "    data_train = data[:split_point]\n",
    "    eval_len = int(len(data[split_point:]) / 2)\n",
    "    data_test = data[split_point:split_point + eval_len]\n",
    "    data_eval = data[split_point+eval_len:]\n",
    "    voc_size = (len(diag_voc.idx2word), len(pro_voc.idx2word), len(med_voc.idx2word))\n",
    "\n",
    "    EPOCH = 30\n",
    "    LR = 0.0005\n",
    "    TEST = False\n",
    "    END_TOKEN = voc_size[2] + 1\n",
    "\n",
    "    model = DMNC(voc_size, device=device)\n",
    "    if TEST:\n",
    "        model.load_state_dict(torch.load(open(os.path.join(\"saved\", model_name, resume_name), 'rb')))\n",
    "    model.to(device=device)\n",
    "    print('parameters', get_n_params(model))\n",
    "\n",
    "    criterion2 = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "\n",
    "    if TEST:\n",
    "        eval(model, data_test, voc_size, 0)\n",
    "    else:\n",
    "        history = defaultdict(list)\n",
    "        for epoch in range(EPOCH):\n",
    "            loss_record1 = []\n",
    "            loss_record2 = []\n",
    "            start_time = time.time()\n",
    "            model.train()\n",
    "            for step, input in enumerate(data_train):\n",
    "                i1_state, i2_state, i3_state = None, None, None\n",
    "                for adm in input:\n",
    "                    loss_target = adm[2] + [END_TOKEN]\n",
    "                    output_logits, i1_state, i2_state, i3_state = model(adm, i1_state, i2_state, i3_state)\n",
    "                    loss = criterion2(output_logits, torch.LongTensor(loss_target).to(device))\n",
    "\n",
    "                    loss_record1.append(loss.item())\n",
    "                    loss_record2.append(loss.item())\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward(retain_graph=True)\n",
    "                    optimizer.step()\n",
    "\n",
    "                llprint('\\rTrain--Epoch: %d, Step: %d/%d' % (epoch, step, len(data_train)))\n",
    "\n",
    "            ddi_rate, ja, prauc, avg_p, avg_r, avg_f1 = eval(model, data_eval, voc_size, epoch)\n",
    "            history['ja'].append(ja)\n",
    "            history['ddi_rate'].append(ddi_rate)\n",
    "            history['avg_p'].append(avg_p)\n",
    "            history['avg_r'].append(avg_r)\n",
    "            history['avg_f1'].append(avg_f1)\n",
    "            history['prauc'].append(prauc)\n",
    "\n",
    "            end_time = time.time()\n",
    "            elapsed_time = (end_time - start_time) / 60\n",
    "            llprint('\\tEpoch: %d, Loss1: %.4f, Loss2: %.4f, One Epoch Time: %.2fm, Appro Left Time: %.2fh\\n' % (epoch,\n",
    "                                                                                                np.mean(loss_record1),\n",
    "                                                                                                np.mean(loss_record2),\n",
    "                                                                                                elapsed_time,\n",
    "                                                                                                elapsed_time * (\n",
    "                                                                                                            EPOCH - epoch - 1)/60))\n",
    "\n",
    "            torch.save(model.state_dict(), open( os.path.join('saved', model_name, 'Epoch_%d_JA_%.4f_DDI_%.4f.model' % (epoch, ja, ddi_rate)), 'wb'))\n",
    "            print('')\n",
    "\n",
    "        dill.dump(history, open(os.path.join('saved', model_name, 'history.pkl'), 'wb'))\n",
    "\n",
    "        # test\n",
    "        torch.save(model.state_dict(), open(\n",
    "            os.path.join('saved', model_name, 'final.model'), 'wb'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
